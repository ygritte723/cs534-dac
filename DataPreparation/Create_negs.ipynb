{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /root/miniconda3/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/root/miniconda3/lib/python3.8/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/root/miniconda3/lib/python3.8/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "from SVLC_learning.negs_and_pos import *\n",
    "from SVLC_learning.color_list import color_list\n",
    "from SVLC_learning.action_list import action_list\n",
    "from SVLC_learning.material_list import material_list\n",
    "from SVLC_learning.size_list import size_list\n",
    "from SVLC_learning.state_list import state_list\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create_Neg initialized successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['a group of people standeatingg eating the raeating holdeatingg umbrellas',\n",
       "  'a group of people standing in the rain watching umbrellas'],\n",
       " ['a group showing people standing in the rain holding umbrellas',\n",
       "  'a group of children standing in the rain holding umbrellas']]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Create_Neg:\n",
    "    def __init__(self):\n",
    "        self.vl_neg_type = [\"color\", \"action\", \"size\", \"state\", \"material\"]  # Negative types\n",
    "        self.num_negs = 2  # Number of negatives from each method\n",
    "        self.auto_neg_types = [\"NOUN\", \"ADP\", \"ADJ\", \"VERB\"]  # POS tags for auto negatives\n",
    "        self.dict_lists = {\n",
    "            \"color\": color_list,\n",
    "            \"action\": action_list,\n",
    "            \"size\": size_list,\n",
    "            \"state\": state_list,\n",
    "            \"material\": material_list,\n",
    "        }\n",
    "        # Initialize Negatives and Negatives_Auto instances\n",
    "        self.negatives = Negatives(self)\n",
    "        self.negatives_auto = Negatives_Auto(self)\n",
    "        print(\"Create_Neg initialized successfully.\")\n",
    "\n",
    "    def create_negs(self, caption):\n",
    "        \"\"\"Generate 2 negatives from each of Negatives and Negatives_Auto.\"\"\"\n",
    "        neg_text = []\n",
    "        negs, negative_captions = self.negatives.create_negs(caption)\n",
    "        negs_auto, negative_auto_captions = self.negatives_auto.create_negs(caption)\n",
    "        neg_text.append(negative_captions)\n",
    "        neg_text.append(negative_auto_captions)\n",
    "\n",
    "        return neg_text\n",
    "caption = \"a group of people standing in the rain holding umbrellas\"\n",
    "create = Create_Neg()\n",
    "create.create_negs(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create_Neg initialized successfully.\n",
      "Processed CSV saved to /root/autodl-tmp/DAC/final_test_1000.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Create_Neg:\n",
    "    def __init__(self):\n",
    "        self.vl_neg_type = [\"color\", \"action\", \"size\", \"state\", \"material\"]  # Negative types\n",
    "        self.num_negs = 2  # Number of negatives from each method\n",
    "        self.auto_neg_types = [\"NOUN\", \"ADP\", \"ADJ\", \"VERB\"]  # POS tags for auto negatives\n",
    "        self.dict_lists = {\n",
    "            \"color\": color_list,\n",
    "            \"action\": action_list,\n",
    "            \"size\": size_list,\n",
    "            \"state\": state_list,\n",
    "            \"material\": material_list,\n",
    "        }\n",
    "        # Initialize Negatives and Negatives_Auto instances\n",
    "        self.negatives = Negatives(self)\n",
    "        self.negatives_auto = Negatives_Auto(self)\n",
    "        print(\"Create_Neg initialized successfully.\")\n",
    "\n",
    "    def create_negs(self, caption):\n",
    "        \"\"\"Generate 2 negatives from each of Negatives and Negatives_Auto.\"\"\"\n",
    "        neg_text = []\n",
    "        negs, negative_captions = self.negatives.create_negs(caption)\n",
    "        negs_auto, negative_auto_captions = self.negatives_auto.create_negs(caption)\n",
    "        neg_text.append(negative_captions)\n",
    "        neg_text.append(negative_auto_captions)\n",
    "\n",
    "        return neg_text\n",
    "\n",
    "    def process_csv(self, input_csv, output_csv):\n",
    "        \"\"\"Read captions from CSV, generate negatives, and save to a new CSV.\"\"\"\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Check for necessary columns\n",
    "        if \"Image File\" not in df.columns or \"Positive Captions\" not in df.columns:\n",
    "            raise ValueError(\"CSV must contain 'Image File' and 'Positive Captions' columns.\")\n",
    "\n",
    "        # Process each row\n",
    "        negative_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            positive_caption = row[\"Positive Captions\"]\n",
    "            negatives = self.create_negs(positive_caption)\n",
    "\n",
    "            # Append negatives to the row\n",
    "            row_data = {\n",
    "                \"Image File\": row[\"Image File\"],\n",
    "                \"Positive Captions\": positive_caption,\n",
    "                \"Negative 1\": negatives[0] if len(negatives) > 0 else None,\n",
    "                \"Negative 2\": negatives[1] if len(negatives) > 1 else None,\n",
    "                \"Negative 3\": negatives[2] if len(negatives) > 2 else None,\n",
    "                \"Negative 4\": negatives[3] if len(negatives) > 3 else None,\n",
    "            }\n",
    "            negative_data.append(row_data)\n",
    "\n",
    "        # Create a new DataFrame with negatives added\n",
    "        negative_df = pd.DataFrame(negative_data)\n",
    "\n",
    "        # Save to the output CSV\n",
    "        negative_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Processed CSV saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_csv = \"/root/autodl-tmp/DAC/test_1000.csv\"\n",
    "output_csv = \"/root/autodl-tmp/DAC/final_test_1000.csv\"\n",
    "\n",
    "# Create the main object\n",
    "create = Create_Neg()\n",
    "\n",
    "# Process the CSV\n",
    "create.process_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenize(\"The blue ultraman car is small and happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   518,  1746, 44519,   550,  1615,   533,  2442,   537,   900,\n",
       "         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,  1746, 49407,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenize(\"The blue green car is small and happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 5, 6, 7, 8, 9, 10], [3, 4, 5, 6, 7, 8, 9, 10]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_mismatched_indices_separated(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dac/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def find_mismatched_indices_separated(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Finds the mismatched indices separately for two embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding1 (list or torch.Tensor): The first embedding.\n",
    "        embedding2 (list or torch.Tensor): The second embedding.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of two lists:\n",
    "            - First list contains the indices for mismatched values in embedding1.\n",
    "            - Second list contains the indices for mismatched values in embedding2.\n",
    "    \"\"\"\n",
    "    # Convert inputs to lists if they are torch tensors\n",
    "    if isinstance(embedding1, torch.Tensor):\n",
    "        embedding1 = embedding1.flatten().tolist()\n",
    "    if isinstance(embedding2, torch.Tensor):\n",
    "        embedding2 = embedding2.flatten().tolist()\n",
    "\n",
    "    # Check the length of embeddings\n",
    "    if len(embedding1) != len(embedding2):\n",
    "        raise ValueError(\"The embeddings must have the same length.\")\n",
    "\n",
    "    # Find mismatched indices\n",
    "    mismatched_indices_1 = []\n",
    "    mismatched_indices_2 = []\n",
    "    for i, (a, b) in enumerate(zip(embedding1, embedding2)):\n",
    "        if a != b:\n",
    "            mismatched_indices_1.append(i)\n",
    "            mismatched_indices_2.append(i)\n",
    "    \n",
    "    return [mismatched_indices_1, mismatched_indices_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_sentence_differences_torch(matrix1, matrix2, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Compute the difference between matching elements of two matrices using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        matrix1 (torch.Tensor): First tensor of shape (320, 512).\n",
    "        matrix2 (torch.Tensor): Second tensor of shape (320, 512).\n",
    "        metric (str): Metric to compute the difference. Options are 'euclidean', 'cosine', or 'manhattan'.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Differences of shape (320, 1).\n",
    "    \"\"\"\n",
    "    if matrix1.shape != matrix2.shape:\n",
    "        raise ValueError(\"Both matrices must have the same shape.\")\n",
    "    \n",
    "    if metric == 'euclidean':\n",
    "        # Compute Euclidean distance for each pair\n",
    "        differences = torch.norm(matrix1 - matrix2, dim=1, keepdim=True)\n",
    "    elif metric == 'cosine':\n",
    "        # Compute Cosine similarity and convert to distance\n",
    "        norm1 = torch.norm(matrix1, dim=1, keepdim=True)\n",
    "        norm2 = torch.norm(matrix2, dim=1, keepdim=True)\n",
    "        dot_product = torch.sum(matrix1 * matrix2, dim=1, keepdim=True)\n",
    "        similarities = dot_product / (norm1 * norm2 + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        differences = 1 - similarities  # Cosine distance\n",
    "    elif metric == 'manhattan':\n",
    "        # Compute Manhattan distance for each pair\n",
    "        differences = torch.sum(torch.abs(matrix1 - matrix2), dim=1, keepdim=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported metric. Use 'euclidean', 'cosine', or 'manhattan'.\")\n",
    "    \n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences shape: torch.Size([320, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example tensors\n",
    "matrix1 = torch.rand(320, 512)\n",
    "matrix2 = torch.rand(320, 512)\n",
    "\n",
    "# Compute differences using Euclidean distance\n",
    "differences = compute_sentence_differences_torch(matrix1, matrix2, metric='euclidean')\n",
    "\n",
    "print(\"Differences shape:\", differences.shape)  # Should print torch.Size([320, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_differences_in_groups(differences, group_size=10):\n",
    "    \"\"\"\n",
    "    Normalize a tensor in groups so that values in each group sum to 1.\n",
    "    \n",
    "    Args:\n",
    "        differences (torch.Tensor): Tensor of shape (N, 1) containing the differences.\n",
    "        group_size (int): Number of items in each group to normalize.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Normalized tensor with the same shape as the input.\n",
    "    \"\"\"\n",
    "    num_items = differences.size(0)\n",
    "    if num_items % group_size != 0:\n",
    "        raise ValueError(\"The number of items must be divisible by the group size.\")\n",
    "    \n",
    "    # Reshape into groups for normalization\n",
    "    reshaped = differences.view(-1, group_size, 1)\n",
    "    \n",
    "    # Compute the sum of each group\n",
    "    group_sums = reshaped.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    group_sums[group_sums == 0] = 1e-8\n",
    "    \n",
    "    # Normalize each group\n",
    "    normalized = reshaped / group_sums\n",
    "    \n",
    "    # Reshape back to original shape\n",
    "    normalized = normalized.view_as(differences)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = normalize_differences_in_groups(differences).view(1, -1)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 320])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = torch.rand(32, 320)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 320])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = matrix * temp\n",
    "result.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
