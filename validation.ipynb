{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dac/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from open_clip import create_model_and_transforms\n",
    "from open_clip import tokenize \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(\"../CC_10000/val_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>quality_captions</th>\n",
       "      <th>LLM_dense_captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://l7.alamy.com/zooms/400a6896682a4578b028...</td>\n",
       "      <td>1435576_3694708615.jpg</td>\n",
       "      <td>quality_captions/1435576_3694708615.json</td>\n",
       "      <td>LLM_dense/1435576_3694708615.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://l7.alamy.com/zooms/889834cf95fd4cf58c7e...</td>\n",
       "      <td>1436456_3127317779.jpg</td>\n",
       "      <td>quality_captions/1436456_3127317779.json</td>\n",
       "      <td>LLM_dense/1436456_3127317779.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://img-aws.ehowcdn.com/560x560p/photos.dem...</td>\n",
       "      <td>851376_2084137130.jpg</td>\n",
       "      <td>quality_captions/851376_2084137130.json</td>\n",
       "      <td>LLM_dense/851376_2084137130.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://image.shutterstock.com/z/stock-photo-c...</td>\n",
       "      <td>1437128_1476564219.jpg</td>\n",
       "      <td>quality_captions/1437128_1476564219.json</td>\n",
       "      <td>LLM_dense/1437128_1476564219.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ak3.picdn.net/shutterstock/videos/5612...</td>\n",
       "      <td>1437550_71712799.jpg</td>\n",
       "      <td>quality_captions/1437550_71712799.json</td>\n",
       "      <td>LLM_dense/1437550_71712799.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url         image_file_name  \\\n",
       "0  http://l7.alamy.com/zooms/400a6896682a4578b028...  1435576_3694708615.jpg   \n",
       "1  http://l7.alamy.com/zooms/889834cf95fd4cf58c7e...  1436456_3127317779.jpg   \n",
       "2  http://img-aws.ehowcdn.com/560x560p/photos.dem...   851376_2084137130.jpg   \n",
       "3  https://image.shutterstock.com/z/stock-photo-c...  1437128_1476564219.jpg   \n",
       "4  https://ak3.picdn.net/shutterstock/videos/5612...    1437550_71712799.jpg   \n",
       "\n",
       "                           quality_captions                 LLM_dense_captions  \n",
       "0  quality_captions/1435576_3694708615.json  LLM_dense/1435576_3694708615.json  \n",
       "1  quality_captions/1436456_3127317779.json  LLM_dense/1436456_3127317779.json  \n",
       "2   quality_captions/851376_2084137130.json   LLM_dense/851376_2084137130.json  \n",
       "3  quality_captions/1437128_1476564219.json  LLM_dense/1437128_1476564219.json  \n",
       "4    quality_captions/1437550_71712799.json    LLM_dense/1437550_71712799.json  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Missing Keys: []\n",
      "Unexpected Keys: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisualTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, _, image_preprocess = create_model_and_transforms(\n",
    "        \"ViT-B/32\",\n",
    "        \"openai\",\n",
    "        precision=\"amp\",\n",
    "        device=\"cpu\",\n",
    "        jit=False,\n",
    "        force_quick_gelu=False,\n",
    "        pretrained_image=False,\n",
    "        image_mean=None,\n",
    "        image_std=None,\n",
    "        lora=4,\n",
    "        freeze_img=False,\n",
    "        kqv_lora=False,\n",
    "    )\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisualTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_attn): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ln): Identity()\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_attn): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (ln): Identity()\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"logs/baseline/checkpoints/epoch_10.pt\", map_location=\"cuda:0\")  ### replace your model weight here.\n",
    "sd = checkpoint[\"state_dict\"]\n",
    "if next(iter(sd.items()))[0].startswith(\"module\"):\n",
    "    sd = {k[len(\"module.\") :]: v for k, v in sd.items()}\n",
    "model.load_state_dict(sd)\n",
    "model.to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_scores_with_loss_batch(model, data, device=\"cuda\"):\n",
    "    # Preprocessing for images\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Adjust size to match model requirements\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    cumulative_loss = 0.0\n",
    "    num_samples = 0\n",
    "    all_logits_per_image = []\n",
    "    all_logits_per_text = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for i in range(0, len(data), 32):  # Batch size of 4 (can be adjusted)\n",
    "            batch_data = data.iloc[i:i+32]\n",
    "            images = []\n",
    "            captions = []\n",
    "\n",
    "            # Load and preprocess images and captions for the batch\n",
    "            for _, row in batch_data.iterrows():\n",
    "                # Load and preprocess image\n",
    "                image_path = \"../CC_10000/validation/\" + row.image_file_name\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                except Exception as e:\n",
    "                    print(\"image corrupted\")\n",
    "                    dummy_array = np.full((224, 224, 3), 128, dtype=np.uint8)  # Gray image\n",
    "                    image = Image.fromarray(dummy_array)\n",
    "                images.append(preprocess(image))\n",
    "\n",
    "                # Load caption text\n",
    "                caption_path = \"../CC_10000/\"+row.quality_captions\t\n",
    "                with open(caption_path, \"r\") as f:\n",
    "                    captions.append(f.read().strip())\n",
    "\n",
    "            # Convert images to tensor and move to device\n",
    "            images = torch.stack(images).to(device)\n",
    "\n",
    "            # Tokenize captions and move to device\n",
    "            tokenized_captions = tokenize(captions).to(device)\n",
    "\n",
    "            # Compute embeddings for all images and texts\n",
    "            image_embeddings = model(images, None)\n",
    "            text_embeddings = model(None, tokenized_captions)\n",
    "\n",
    "            # Normalize embeddings\n",
    "            image_embeddings = image_embeddings / torch.norm(image_embeddings, dim=1, keepdim=True)\n",
    "            text_embeddings = text_embeddings / torch.norm(text_embeddings, dim=1, keepdim=True)\n",
    "\n",
    "            # Compute similarity logits\n",
    "            logits_per_image = torch.matmul(image_embeddings, text_embeddings.T)  # Image-to-text similarities\n",
    "            logits_per_text = logits_per_image.T  # Text-to-image similarities\n",
    "\n",
    "            # Store logits for potential later use\n",
    "            all_logits_per_image.append(logits_per_image)\n",
    "            all_logits_per_text.append(logits_per_text)\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            labels = torch.arange(logits_per_image.shape[0], device=device)  # Ground truth indices\n",
    "            loss_per_image = F.cross_entropy(logits_per_image, labels)\n",
    "            loss_per_text = F.cross_entropy(logits_per_text, labels)\n",
    "            total_loss = (loss_per_image + loss_per_text) / 2\n",
    "\n",
    "            # Accumulate loss\n",
    "            cumulative_loss += total_loss.item() * logits_per_image.shape[0]\n",
    "            num_samples += logits_per_image.shape[0]\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = cumulative_loss / num_samples\n",
    "\n",
    "    # Return metrics\n",
    "    metrics = {\n",
    "        \"avg_loss\": avg_loss,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_scores_with_loss_batch(model, validation_data, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
